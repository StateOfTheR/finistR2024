---
title: "Introduction to `nimble`"
format: html
editor_options: 
  chunk_output_type: console
---

# Required packages for this tutorial {.unnumbered}

```{r}
#| label: packages_nimble_section
#| warning: false
#| message: false
#| cache: false
library(compareMCMCs)
library(ggmcmc)
library(nimble)
library(nimbleHMC)
library(tidyverse)
library(rjags)

```



# Short presentation of `nimble`

First, a very good reference: https://r-nimble.org/html_manual/cha-welcome-nimble.html

The `nimble` package is:

- A system for writing statistical models flexibly (based on `bugs` and `jags`).
- A library of inference algorithms (MCMC, HMC, Laplace approximation).
- A compiler that generates and compiles C++ for your models and algorithms without knowing it is C++.

There is a `nimble` language with the basics for building statistical models (e.g. the key probability distributions, some key transformation functions).

But, this remains limited and to have full flexibility, one need to build its own function > **can be very challenging**

# A simple example with functions being available in `nimble`

## Defining a negative binomial model

```{r}
#| label: example1_data
data_ex1 <- rnbinom(n = 1000, prob = 0.4, size = 12)
```



```{r}
#| label: code_neg_bin
code_neg_bin <- nimbleCode({
  # Observation model
  for(i in 1:n){# n is never defined before, it will be a constant
    y[i] ~ dnbinom(prob, theta)
  }
  # Alternative vectorized formulation 
  # y[1:n] ~ dnbinom(prob, theta)
  # PRIORS
  prob ~ dunif(0, 1)
  theta ~ dexp(0.1)
})
```

Note that in this code, nothing distinguishes observed data from unknown (or latent variables).
The order of lines has no importance as everything will be compiled afterwards

## Defining the nimble model

```{r}
#| label: model_neg_bin
model_neg_bin <- nimbleModel(code = code_neg_bin, 
                    name = "Negative binomial", 
                    constants = list(n = length(data_ex1)),
                    data = list(y = data_ex1),
                    inits = list(prob = 0.5, theta = 1))
```

```{r}
#| label: posterior_samples
#| cache: true
posterior_samples_neg_bin <- nimbleMCMC(model_neg_bin, 
                                        nchains = 3, 
                                        niter = 10000, 
                                        thin = 10, 
                                        nburnin = 1000)
```

## Exploring the results

```{r}
#| label: str_posterior_samples
str(posterior_samples_neg_bin)
```

```{r}
#| label: custom_formatted_results
formatted_results <- imap_dfr(posterior_samples_neg_bin, 
     function(x, nm){
       as.data.frame(x) %>% 
         rowid_to_column(var = "Iteration") %>% 
         mutate(Chain = str_remove(nm, "chain"))
     }) %>% 
  pivot_longer(cols = -c("Iteration", "Chain"),
               names_to = "Parameter", 
               values_to = "value")
```

```{r}
#| label: plot
ggplot(formatted_results) +
  aes(x = Iteration,
      y = value, color = Chain) +
  facet_wrap(~Parameter, scales = "free") +
  geom_line() +
  labs(x = "Sample ID", y = "Parameter value", color = "")
```

## Package for automatic formatting

```{r}
#| label: posterior_samples_coda
#| cache: true
posterior_samples_neg_bin <- nimbleMCMC(model_neg_bin, 
                                        nchains = 3, 
                                        niter = 10000, 
                                        thin = 10, 
                                        nburnin = 1000,
                                        samplesAsCodaMCMC = TRUE)
```

```{r}
#| label: str_posterior_samples_coda
str(posterior_samples_neg_bin)
```

```{r}
#| label: coda_formatted_results
formatted_results <- ggs(posterior_samples_neg_bin)
formatted_results
```

## Defining a `nimbleFunction`

```{r}
get_prob <- nimbleFunction(
    run = function(log_mu = double(0),
                   log_theta = double(0)) { # type declarations
        z <- log_theta - log_mu
        output <- 1 / (1 + exp(-z))
        return(output)
        returnType(double(0))  # return type declaration
    })
get_prob(log(18), log(12))
```


```{r}
#| label: code_neg_bin_alternatif
#| cache: true
code_alternatif <- nimbleCode({
  # Observation model
  for(i in 1:n){# n is never defined before, it will be a constant
    y[i] ~ dnbinom(prob, theta)
  }
  # Alternative vectorized formulation 
  # y[1:n] ~ dnbinom(prob, theta)
  # PRIORS
  log_mu ~ dnorm(0, 1)
  theta ~ dexp(0.1)
  # Quantites deterministes
  log_theta <- log(theta)
  prob <- get_prob(log_mu = log_mu, log_theta = log_theta)
})

model_alternatif <- nimbleModel(code = code_alternatif, 
                    name = "Alternative negative binomial", 
                    constants = list(n = length(data_ex1)),
                    data = list(y = data_ex1),
                    inits = list(mu = 0.5, theta = 1))

posterior_samples_alternatif <- nimbleMCMC(model_alternatif, 
                                        nchains = 3, 
                                        niter = 10000, 
                                        thin = 10, 
                                        nburnin = 1000,
                                        monitors = c("prob", "theta"),
                                        samplesAsCodaMCMC = TRUE)
```

```{r}
posterior_samples_alternatif %>% 
  ggs() %>% 
  ggplot() +
  aes(x = Iteration,
      y = value, color = factor(Chain)) +
  facet_wrap(~Parameter, scales = "free") +
  geom_line() +
  labs(x = "Sample ID", y = "Parameter value", color = "")
```


## Alternative MCMC sampler

One big strength of `nimble` are the several samplers that are available in the package.

## Conjuguate priors

First, nimble is able to identify conjugate priors and make the exact computation of the posterior [link](https://r-nimble.org/html_manual/cha-mcmc.html#conjugate-gibbs-samplers).

## HMC algorithm

`nimble` provides support for Hamiltonian Monte Carlo (HMC) and compute the derivatives of the likelihood through automatic differentiation. The `nimbleHMC` package implement two versions of No-U-Turn (NUTS) HMC sampling: the standard one developed in Hoffman and Gelman ([link](https://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf)) and an updated on with improved adaptation routines and convergence criteria, which matches the HMC sampler of `STAN`.

In order to allow an algorithm to use AD for a specific model, that model must be created with buildDerivs = TRUE.

```{r}
#| label: HMC_code
#| cache: true

# Build model with nimble
model_neg_bin_HMC <- nimbleModel(code = code_neg_bin, 
                                 name = "Negative binomial", 
                                 constants = list(n = length(data_ex1)),
                                 data = list(y = data_ex1),
                                 inits = list(prob = 0.5, theta = 1),
                                 calculate = FALSE, buildDerivs = TRUE) # This is the line required for running HMC

C_model_neg_bin_HMC <- compileNimble(model_neg_bin_HMC) # Compile the model (they require this for the compilation of the HMC object)

# Build the MCMC algorithm which applies HMC sampling
HMC <- buildHMC(C_model_neg_bin_HMC)

# Careful here, when the model has random effects
# HMC requires to set values in the model before running the algorithm
# One solution is to simulate with the model and set the model with these values
# See : https://r-nimble.org/html_manual/cha-mcmc.html#subsec:HMC-example
# Here, as the model is simple, there is no need for this and everything is handled withing nimble/nimbleHMC

## Then everything is standard in nimble
CHMC <- compileNimble(HMC) # Compile the HMC model/algo
samples <- runMCMC(CHMC, niter = 1000, nburnin = 500) # Short run for illustration
summary(coda::as.mcmc(samples)) # Summary of the estimates
```

And there are plenty of others samplers:

- Particle filters / sequential Monte Carlo and iterated filtering (package `nimbleSMC`)

- Monte Carlo Expectation Maximization (MCEM)

See [link](https://r-nimble.org/html_manual/cha-algos-provided.html#cha-algos-provided)


## The laplace approximation

`nimble` also implements the Laplace approximation. But be careful, it performs maximum likelihood estimation. This is not the same as `INLA` (fully bayesian approach), but more like `TMB` (or `glmmTMB`- maximum likelihood estimation through Laplace approximation and automatic differentiation).

```{r}
#| label: Laplace_code
#| cache: true

# We need the derivatives to build the Laplace algorithm
# so we take the object model_neg_bin_HMC built previously
model_laplace <- buildLaplace(model_neg_bin_HMC)
Cmodel_laplace <- compileNimble(model_laplace)

# Get the Laplace approximation for one set of parameter values.
Cmodel_laplace$calcLaplace(c(0.5,0.5)) 

 # Get the corresponding gradient.
Cmodel_laplace$gr_Laplace(c(0.5,0.5))

# Search the (approximate) MLE
MLE <- Cmodel_laplace$findMLE(c(0.5,0.5)) # Find the (approximate) MLE.
MLE$par

# Get log-likelihood value
MLE$value
# And output summaries
Cmodel_laplace$summary(MLE)

```

N.b this example is only for illustration of the code. The Laplace approximation is relevant only when there are random effects in the model (which is not the case here).

For a full example see [link](https://r-nimble.org/html_manual/cha-AD.html#sec:AD-laplace)

## Comparing MCMC algorithms

One can compare several algorithms through the package `compareMCMCs`. It is possible to compare several algorithms internal to `nimble` with those from `jags` (or even `STAN`) algorithms. An example below for `nimble` and `STAN`.

```{r}
#| label: compareMCMC_code
#| cache: true

# This model code will be used for both nimble and JAGS
modelInfo <- list(
  code = code_neg_bin,
  constants = list(n = length(data_ex1)),
  data = list(y = data_ex1),
  inits = list(prob = 0.5, theta = 1)
)

# Here is a custom MCMC configuration function for nimble
configure_nimble_slice <- function(model) {
  configureMCMC(model, onlySlice = TRUE)
}

# Here is the call to compareMCMCs
res <- compareMCMCs(modelInfo,
                    MCMCs = c('jags',
                              'nimble',       # nimble with default samplers
                              'nimble_slice', # nimble with slice samplers
                              'nimble_hmc'
                              ),
                    nimbleMCMCdefs = 
                      list(nimble_slice = 'configure_nimble_slice'),
                    MCMCcontrol = list(inits = list(prob = 0.5, theta = 1),
                                       niter = 10000,
                                       burnin = 1000))

make_MCMC_comparison_pages(res, modelName = 'code_neg_bin',dir = "/tmp/",
                           control = list(res = 75))

```
