---
title: "Setting up simulations in R"
author:
  - Julie Aubert
  - Caroline Cognot
  - Annaïg De Walsche
  - Cédric Midoux
  - Aymeric Stamm
  - Florian Teste
date: 2024-08-19
---

```{r setup}
#| include: false
library(ggplot2)
library(simpr)
```

## List of packages for data simulation

-   [**simulator**](https://jacobbien.github.io/simulator/)
-   [**simpr**](https://statisfactions.github.io/simpr/)
-   [**DeclareDesign**](https://declaredesign.org/r/declaredesign/)
-   [**MonteCarlo**](https://github.com/FunWithR/MonteCarlo)
-   [**simChef**](https://yu-group.github.io/simChef/index.html)
-   [**simEngine**](https://cran.r-project.org/web/packages/SimEngine/vignettes/SimEngine.html)

## How to choose ?

-   Number of dependencies
-   Number of reverse dependencies
-   Date of latest commit
-   Date of last release
-   Is it on CRAN or only on Github?
-   Who developed it?
-   Philosophy

| Name  | Version | #deps | #rev deps | Latest commit | Latest release | Doc | On CRAN? | Developers                   |
|---------------------------------------------------------------------------------------------|---------|------------------------|--------------------------------|-----------------------|------------------------|-----------------------|----------|------------------------------|
| [**DeclareDesign**](https://declaredesign.org/r/declaredesign/)                             | 1.0.10  | 2                      | 1                              | 2024-04-13            | 2024-04-21             |                       | Yes      | Graeme Blair                 |
| [**MonteCarlo**](https://github.com/FunWithR/MonteCarlo)                                    | 1.0.6   | 6                      | 0                              | 2019-01-31            | 2019-01-31             |                       | Yes      | Christian Hendrik Leschinski |
| [**simChef**](https://yu-group.github.io/simChef/index.html)                                | 0.1.0   | 22                     | 0                              | 2024-03-20            | NA                     |                       | No       | Tiffany Tang, James Duncan   |
| [**simEngine**](https://cran.r-project.org/web/packages/SimEngine/vignettes/SimEngine.html) | 1.4.0   | 6                      | 0                              | 2024-04-13            | 2024-04-04             |                       | Yes      | Avi Kenny, Charles Wolock    |
| [**simpr**](https://statisfactions.github.io/simpr/)                                        | 0.2.6   | 11                     | 0                              | 2024-07-16            | 2023-04-26             |                       | Yes      | Ethan Brown                  |
| [**simulator**](https://jacobbien.github.io/simulator/)                                     | 0.2.5   | 1                      | 0                              | 2023-02-02            | 2023-02-04             |                       | Yes      | Jacob Bien                   |

## [**simpr**](https://statisfactions.github.io/simpr/)

::: callout-note
## Problem

-   Study: Pre-post comparison of the “Triglicious” intervention;
-   Research question: Did Triglicious improve students’ math scores?
-   Method: Paired t-test
:::

### Usual base R solution

```{r base-r}
## Set up parameters
ns <- c(100L, 150L, 200L)
mean_diffs <- c(10, 20, 30)
sds <- c(50, 100)
reps <- 10L

## Bring together into data frame
results_template <- expand.grid(
  n = ns, 
  mean_diff = mean_diffs, 
  sd = sds, 
  p.value = NA_real_
)
base_r_sim <- results_template[rep(1:nrow(results_template), each = reps), ]

## Loop over rows of the data frame and calculate the p-value
for (i in 1:nrow(results_template)) {
  params <- base_r_sim[i,]
  pre <- rnorm(params$n, 0, params$sd)
  post <- pre + rnorm(params$n, params$mean_diff, params$sd)
  base_r_sim$p.value[i] <- t.test(pre, post)$p.value
}

## Display table output
DT::datatable(base_r_sim)
```

What [**simpr**](https://statisfactions.github.io/simpr/) author advocates is bad:

-   Most important pieces (data generating process, model specification, definitions, varying parameters) are hidden;
-   What if there is an error?
-   What about parallelization?
-   Is this code sufficiently readable? Without the comments?

### Solution via [**simpr**](https://statisfactions.github.io/simpr/)

```{r simpr-spec}
## Specify pre and post scores that differ by a given amount
specify(
  pre  = ~ rnorm(n, 0, sd), 
  post = ~ pre + rnorm(n, mean_diff, sd)) |> 
  ## Define parameters that can be varied
  define(n = 100, mean_diff = 10, sd = 50) |> 
  ## Generate datasets
  generate(100) |> 
  ## Fit datasets
  fit(t = ~t.test(post, pre, paired = TRUE)) |> 
  ## Collect results
  tidy_fits() |> 
  DT::datatable()
```

### A complete solution with varying parameters

```{r simpr-vary}
sim_vary <- specify(
  pre  = ~ rnorm(n, 0, sd),
  post = ~ pre + rnorm(n, mean_diff, sd)
) |> 
  define(
    n = c(100, 150, 200), 
    mean_diff = c(10, 20, 30), 
    sd = c(50, 100)
  ) |> 
  generate(100, .progress = TRUE) |> 
  fit(t = ~t.test(post, pre, paired = TRUE)) |> 
  tidy_fits()

sim_vary |> 
  dplyr::group_by(n, mean_diff, sd) |> 
  dplyr::summarize(Power = mean(p.value < 0.05)) |> 
  ggplot(aes(n, Power)) + 
  geom_col() + 
  facet_grid(rows = vars(sd), cols = vars(mean_diff)) + 
  theme_bw()
```

### Philosophy

The [**simpr**](https://statisfactions.github.io/simpr/) workflow, inspired by the [**infer**](https://infer.tidymodels.org) package, distills a simulation study into five primary steps:

-   `specify()` your data-generating process;
-   `define()` parameters that you want to systematically vary across your simulation design (e.g. n, effect size);
-   `generate()` the simulation data;
-   `fit()` models to your data (e.g. `lm()`);
-   `tidy_fits()` for consolidating results using `broom::tidy()`, such as computing power or Type I Error rates.

### Reproducible workflows

-   Same seed, same results;
-   Can regenerate just a *specific subset* to see what happened in that particular dataset or fit;
-   Useful in debugging and diagnosing unexpected results, etc.

#### Filtering full simulation

```{r simpr-full}
set.seed(500)

specify(a = ~ runif(6)) |> 
  generate(3) |> 
  dplyr::filter(.sim_id == 3)
```

#### Simulate subset only

```{r simpr-subset}
set.seed(500)

specify(a = ~ runif(6)) |> 
  generate(3, .sim_id == 3)
```

#### Benchmarking

```{r simpr-bench}
set.seed(500)

bench::mark(
  all = specify(a = ~ runif(6)) |> 
    generate(1000) |> 
    dplyr::filter(.sim_id == 1000),
  subset = specify(a = ~ runif(6)) |> 
    generate(1000, .sim_id == 1000),
  check = FALSE, min_iterations = 10L, relative = TRUE
)
```

### Other features

::: callout-tip
## Data munging

Add `per_sim() |>` after `generate()` in your simulation pipeline and then any tidyverse function that will apply to every simulation dataset:

```{r simpr-other}
#| eval: false
specify(
  pre  = ~ rnorm(n, 0, sd), 
  post = ~ pre + rnorm(n, mean_diff, sd)
) |> 
  define(
    n = c(100, 150, 200), 
    mean_diff = c(10, 20, 30), 
    sd = c(50, 100)
  ) |> 
  generate(1000, .progress = TRUE) |> 
  ## Apply tidyverse functions to every simulation dataset
  per_sim() |> 
  ## Mutate to add a range restriction
  dplyr::mutate(dplyr::across(dplyr::everything(), dplyr::case_when(
    pre >  100 ~ 100,
    pre < -100 ~ -100,
    .default   ~ pre
  ))) |> 
  fit(t = ~ t.test(post, pre, paired = TRUE)) |> 
  tidy_fits()
```
:::

::: callout-tip
## Error handling

-   Can change error handling to keep going with simulation, stop simulation, or to skip warnings;
-   Debug and recovery options to enter into simulation during error.
:::

::: callout-tip
## Built-in parallelization

Just add

```{r}
#| eval: false
library(future)
plan(multisession, workers = 6) # or however many cores are reasonable to use
```

and your simulation pipeline (actually the `generate()` function) will run in parallel.
:::

### Pros & Cons

::: columns
::: column
::: {.callout-note icon="false"}
## Pros

-   tidyverse friendly;
-   beginner friendly;
-   Reproducibility, error handling built in;
-   General-purpose, customizable and can handle arbitrary R code.
:::
:::

::: column
::: {.callout-important icon="false"}
## Cons

-   Likely not as fast/optimized as some alternatives;
-   Not as customizable/powerful as **DeclareDesign**;
-   Not specifically set up for any particular application (no MC errors, plots, reports, specific models…).
:::
:::
:::

# Simulator

This is a package on the CRAN. It is described in a 2016 paper by Jacob Bien.
Last update on GitHub : last year (so, 2023).

## Getting started

The function "create", with a directory that does not exist, will create
the directory with 5 files :

-   eval_functions.R : contains metrics to be evaluated
-   main.R : main code to run
-   method_functions.R : methods to run
-   model_functions.R : define the models.
-   writeup.Rmd

```{r }
library(simulator)
dir <- "./sims_simulator"
create(dir)
```

```{r}
setwd(dir)

list.files()
```

"On a typical project, one starts by defining a model in
model_functions.R, one or two methods in method_functions.R, and a few
metrics in eval_functions.R, and then one runs the code in main.R. After
looking at some of the results, one might add an additional model or
method or metric. One then returns to main.R, adds some additional lines
specifying that the additional components should be run as well and
looks at some more results.

The simplest way to look at results is by using the plot functions
"plot_eval", "plot_evals" and "plot_evals_by." In situations where you
wish to investigate results more deeply than just looking at aggregated
plots, one can use the functions "model", "draws", "output", and "evals"
to get at all objects generated through the course of the simulation."

The "create" function also create the template in the different files :

### what is in model_functions

```{r}

make_my_model <- function(n, prob) {
  new_model(name = "contaminated-normal",
            label = sprintf("Contaminated normal (n = %s, prob = %s)", n, prob),
            params = list(n = n, mu = 2, prob = prob),
            simulate = function(n, mu, prob, nsim) {
              # this function must return a list of length nsim
              contam <- runif(n * nsim) < prob
              x <- matrix(rep(NA, n * nsim), n, nsim)
              x[contam] <- rexp(sum(contam))
              x[!contam] <- rnorm(sum(!contam))
              x <- mu + x # true mean is mu
              return(split(x, col(x))) # make each col its own list element
            })
}
```

Define a model from its different components with "new_model" :

-   name
-   label : what will be printed in the tables later probably ?
-   param : a list of different parameters for the model
-   simulate : a function of the parameters, that returns nsim
    simulations.

### what is in method_functions

```{r}
my_method <- new_method("my-method", "My Method",
                        method = function(model, draw) {
                          list(fit = median(draw))
                        })

their_method <- new_method("their-method", "Their Method",
                           method = function(model, draw) {
                             list(fit = mean(draw))
                           })

```

Define methods to be used on the model. The function "new_method" has
for arguments a name (for R), a pretty name, and the "method" named arg
for the computation we want.

### what is in eval_functions

```{r}
his_loss <- new_metric("hisloss", "His loss function",
                        metric = function(model, out) {
                          return((model$mu - out$fit)^2)
})

her_loss <- new_metric("herloss", "Her loss function",
                        metric = function(model, out) {
                          return(abs(model$mu - out$fit))
                        })

```

Metric objects : shows how to compare model object and output of the
method (method used on sim) object.

### what is in main

```{r}
setwd(dir)

# This is the main simulator file


library(simulator) # this file was created under simulator version 0.2.5

source("model_functions.R")
source("method_functions.R")
source("eval_functions.R")

## @knitr init

name_of_simulation <- "normal-mean-estimation-with-contamination"

## @knitr main

sim <- new_simulation(name = name_of_simulation,
                      label = "Mean estimation under contaminated normal") %>%
  generate_model(make_my_model, seed = 123,
                 n = 50,
                 prob = as.list(seq(0, 1, length = 6)),
                 vary_along = "prob") %>%
  simulate_from_model(nsim = 10) %>%
  run_method(list(my_method, their_method)) %>%
  evaluate(list(his_loss, her_loss))

## @knitr plots

plot_eval_by(sim, "hisloss", varying = "prob")

## @knitr tables

tabulate_eval(sim, "herloss", output_type = "markdown",
              format_args = list(digits = 1))

```

main calls the different files.

? Can plot_eval_by be used for different metrics at once ? ? Can
tabulate_eval be used for different metrics at once ?

## Important functions :

- new_model()
- new_method()
- new_metric()
- new_simulation
- generate_model
-  simulate_from_model
- run_method
-  evaluate
- plot_eval, plot_eval_by, tabulate_eval.



## Final notes :

This is not really a package that codes a method, but instead it
proposes an architecture to store your codes, output simulations,
results, etc.



PROs :

-   any model possible, if you can write it !
-   possible to iterate over parameter with pretty pipes
-   parallel possible, because you choose what you use
-   stores all results in the storage with increasing depth:

  - *files*
  
    -   *name_of_model*
    
        -   *name_of_first_param\_value*
        
            -   *name_of2nd_param\_value ... model.Rdata* out :
                stores all sims
                
                -   *r?.Rdata*

CONs :

-   Not an usual way to code in R, and not easy to explain. create the
    directory with the "create" function. Then, add the different
    functions, methods, models... in the corresponding files.
-   Mixes the code of the package/template with your own code.
-   stores all results in this neat way BUT if too many parameters, may exceed the depth allowed.
