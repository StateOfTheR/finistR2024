---
title: "Setting up simulations in R"
author:
  - Julie Aubert
  - Caroline Cognot
  - Annaïg De Walsche
  - Cédric Midoux
  - Pierre Neuvial
  - Aymeric Stamm
  - Florian Teste
date: 2024-08-19
---

```{r setup}
#| include: false
library(ggplot2)
library(simpr)
library(SimEngine)
```

## Overview

We explore several R packages for generate or structure simulations. 
Most statistical simulations studies includes different steps : generate data/ run one or several methods using simulated data / compare results.

List of packages for data simulation

-   [**simulator**](https://jacobbien.github.io/simulator/)
-   [**simpr**](https://statisfactions.github.io/simpr/)
-   [**DeclareDesign**](https://declaredesign.org/r/declaredesign/)
-   [**MonteCarlo**](https://github.com/FunWithR/MonteCarlo)
-   [**simChef**](https://yu-group.github.io/simChef/index.html)
-   [**simEngine**](https://cran.r-project.org/web/packages/SimEngine/vignettes/SimEngine.html)

## How to choose ?

-   Number of dependencies
-   Number of reverse dependencies
-   Date of latest commit
-   Date of last release
-   Is it on CRAN or only on Github?
-   Who developed it?
-   Philosophy

| Name                                                                                        | Version | #deps | #rev deps | Latest commit | Latest release | Doc | On CRAN? | Developers                   |
|--------------|--------|--------|--------|--------|--------|--------|--------|--------|
| [**DeclareDesign**](https://declaredesign.org/r/declaredesign/)                             | 1.0.10  | 2     | 1         | 2024-04-13    | 2024-04-21     |     | Yes      | Graeme Blair                 |
| [**MonteCarlo**](https://github.com/FunWithR/MonteCarlo)                                    | 1.0.6   | 6     | 0         | 2019-01-31    | 2019-01-31     |     | Yes      | Christian Hendrik Leschinski |
| [**simChef**](https://yu-group.github.io/simChef/index.html)                                | 0.1.0   | 22    | 0         | 2024-03-20    | NA             |     | No       | Tiffany Tang, James Duncan   |
| [**simEngine**](https://cran.r-project.org/web/packages/SimEngine/vignettes/SimEngine.html) | 1.4.0   | 6     | 0         | 2024-04-13    | 2024-04-04     |     | Yes      | Avi Kenny, Charles Wolock    |
| [**simpr**](https://statisfactions.github.io/simpr/)                                        | 0.2.6   | 11    | 0         | 2024-07-16    | 2023-04-26     |     | Yes      | Ethan Brown                  |
| [**simulator**](https://jacobbien.github.io/simulator/)                                     | 0.2.5   | 1     | 0         | 2023-02-02    | 2023-02-04     |     | Yes      | Jacob Bien                   |

# [**DeclareDesign**](https://declaredesign.org/r/declaredesign/)

The `DeclareDesign` package is dedicated to *experimental design*. It also makes it possible to simulate an experimental design of interest, in order to understand the properties of this design. As its goal is not to evaluate computational methods via simulations, it does not address the question of interest and we did not evaluate it further.

The underlying experimental design principles are described in the companion book: https://book.declaredesign.org/

# [**simpr**](https://statisfactions.github.io/simpr/)

::: callout-note
## Problem

-   Study: Pre-post comparison of the “Triglicious” intervention;
-   Research question: Did Triglicious improve students’ math scores?
-   Method: Paired t-test
:::

## Usual base R solution

```{r base-r}
## Set up parameters
ns <- c(100L, 150L, 200L)
mean_diffs <- c(10, 20, 30)
sds <- c(50, 100)
reps <- 10L

## Bring together into data frame
results_template <- expand.grid(
  n = ns, 
  mean_diff = mean_diffs, 
  sd = sds, 
  p.value = NA_real_
)
base_r_sim <- results_template[rep(1:nrow(results_template), each = reps), ]

## Loop over rows of the data frame and calculate the p-value
for (i in 1:nrow(results_template)) {
  params <- base_r_sim[i,]
  pre <- rnorm(params$n, 0, params$sd)
  post <- pre + rnorm(params$n, params$mean_diff, params$sd)
  base_r_sim$p.value[i] <- t.test(pre, post)$p.value
}

## Display table output
DT::datatable(base_r_sim)
```

What is bad according to [**simpr**](https://statisfactions.github.io/simpr/) authors:

-   Most important pieces (data generating process, model specification, definitions, varying parameters) are hidden;
-   What if there is an error?
-   What about parallelization?
-   Is this code sufficiently readable? Without the comments?

## Solution via [**simpr**](https://statisfactions.github.io/simpr/)

```{r simpr-spec}
## Specify pre and post scores that differ by a given amount
specify(
  pre  = ~ rnorm(n, 0, sd), 
  post = ~ pre + rnorm(n, mean_diff, sd)) |> 
  ## Define parameters that can be varied
  define(n = 100, mean_diff = 10, sd = 50) |> 
  ## Generate datasets
  generate(100) |> 
  ## Fit datasets
  fit(t = ~t.test(post, pre, paired = TRUE)) |> 
  ## Collect results
  tidy_fits() |> 
  DT::datatable()
```

## A complete solution with varying parameters

```{r simpr-vary}
sim_vary <- specify(
  pre  = ~ rnorm(n, 0, sd),
  post = ~ pre + rnorm(n, mean_diff, sd)
) |> 
  define(
    n = c(100, 150, 200), 
    mean_diff = c(10, 20, 30), 
    sd = c(50, 100)
  ) |> 
  generate(100, .progress = TRUE) |> 
  fit(t = ~t.test(post, pre, paired = TRUE)) |> 
  tidy_fits()

sim_vary |> 
  dplyr::group_by(n, mean_diff, sd) |> 
  dplyr::summarize(Power = mean(p.value < 0.05)) |> 
  ggplot(aes(n, Power)) + 
  geom_col() + 
  facet_grid(rows = dplyr::vars(sd), cols = dplyr::vars(mean_diff)) + 
  theme_bw()
```

## Philosophy

The [**simpr**](https://statisfactions.github.io/simpr/) workflow, inspired by the [**infer**](https://infer.tidymodels.org) package, distills a simulation study into five primary steps:

-   `specify()` your data-generating process;
-   `define()` parameters that you want to systematically vary across your simulation design (e.g. n, effect size);
-   `generate()` the simulation data;
-   `fit()` models to your data (e.g. `lm()`);
-   `tidy_fits()` for consolidating results using `broom::tidy()`, such as computing power or Type I Error rates.

## Reproducible workflows

-   Same seed, same results;
-   Can regenerate just a *specific subset* to see what happened in that particular dataset or fit;
-   Useful in debugging and diagnosing unexpected results, etc.

### Filtering full simulation

```{r simpr-full}
set.seed(500)

specify(a = ~ runif(6)) |> 
  generate(3) |> 
  dplyr::filter(.sim_id == 3)
```

### Simulate subset only

```{r simpr-subset}
set.seed(500)

specify(a = ~ runif(6)) |> 
  generate(3, .sim_id == 3)
```

### Benchmarking

```{r simpr-bench}
set.seed(500)

bench::mark(
  all = specify(a = ~ runif(6)) |> 
    generate(1000) |> 
    dplyr::filter(.sim_id == 1000),
  subset = specify(a = ~ runif(6)) |> 
    generate(1000, .sim_id == 1000),
  check = FALSE, min_iterations = 10L, relative = TRUE
)
```

## Other features

::: callout-tip
### Data munging

Add `per_sim() |>` after `generate()` in your simulation pipeline and then any tidyverse function that will apply to every simulation dataset:

```{r simpr-other}
#| eval: false
specify(
  pre  = ~ rnorm(n, 0, sd), 
  post = ~ pre + rnorm(n, mean_diff, sd)
) |> 
  define(
    n = c(100, 150, 200), 
    mean_diff = c(10, 20, 30), 
    sd = c(50, 100)
  ) |> 
  generate(1000, .progress = TRUE) |> 
  ## Apply tidyverse functions to every simulation dataset
  per_sim() |> 
  ## Mutate to add a range restriction
  dplyr::mutate(dplyr::across(dplyr::everything(), dplyr::case_when(
    pre >  100 ~ 100,
    pre < -100 ~ -100,
    .default   ~ pre
  ))) |> 
  fit(t = ~ t.test(post, pre, paired = TRUE)) |> 
  tidy_fits()
```
:::

::: callout-tip
### Error handling

-   Can change error handling to keep going with simulation, stop simulation, or to skip warnings;
-   Debug and recovery options to enter into simulation during error.
:::

::: callout-tip
### Built-in parallelization

Just add

```{r}
#| eval: false
library(future)
plan(multisession, workers = 6) # or however many cores are reasonable to use
```

and your simulation pipeline (actually the `generate()` function) will run in parallel.
:::

## Pros & Cons of simpr

::: columns
::: column
::: {.callout-note icon="false"}
### Pros

-   tidyverse friendly;
-   beginner friendly;
-   Reproducibility, error handling built in;
-   General-purpose, customizable and can handle arbitrary R code.
:::
:::

::: column
::: {.callout-important icon="false"}
### Cons

-   Likely not as fast/optimized as some alternatives;
-   Not as customizable/powerful as **DeclareDesign**;
-   Not specifically set up for any particular application (no MC errors, plots, reports, specific models…).
:::
:::
:::

# Simulator

This is a package on the CRAN. It is described in a 2016 paper by Jacob Bien. Last update on GitHub : last year (so, 2023).

## Getting started

The function "create", with a directory that does not exist, will create the directory with 5 files :

-   eval_functions.R : contains metrics to be evaluated
-   main.R : main code to run
-   method_functions.R : methods to run
-   model_functions.R : define the models.
-   writeup.Rmd

```{r simulator-dir}
library(simulator)
dir <- "./sims_simulator"
if (!file.exists(dir)) {
  create(dir)
}
```

```{r}
setwd(dir)

list.files()
```

"On a typical project, one starts by defining a model in model_functions.R, one or two methods in method_functions.R, and a few metrics in eval_functions.R, and then one runs the code in main.R. After looking at some of the results, one might add an additional model or method or metric. One then returns to main.R, adds some additional lines specifying that the additional components should be run as well and looks at some more results.

The simplest way to look at results is by using the plot functions "plot_eval", "plot_evals" and "plot_evals_by." In situations where you wish to investigate results more deeply than just looking at aggregated plots, one can use the functions "model", "draws", "output", and "evals" to get at all objects generated through the course of the simulation."

The "create" function also create the template in the different files :

### what is in model_functions

```{r}

make_my_model <- function(n, prob) {
  new_model(name = "contaminated-normal",
            label = sprintf("Contaminated normal (n = %s, prob = %s)", n, prob),
            params = list(n = n, mu = 2, prob = prob),
            simulate = function(n, mu, prob, nsim) {
              # this function must return a list of length nsim
              contam <- runif(n * nsim) < prob
              x <- matrix(rep(NA, n * nsim), n, nsim)
              x[contam] <- rexp(sum(contam))
              x[!contam] <- rnorm(sum(!contam))
              x <- mu + x # true mean is mu
              return(split(x, col(x))) # make each col its own list element
            })
}
```

Define a model from its different components with "new_model" :

-   name
-   label : what will be printed in the tables later probably ?
-   param : a list of different parameters for the model
-   simulate : a function of the parameters, that returns nsim simulations.

### what is in method_functions

```{r}
my_method <- new_method("my-method", "My Method",
                        method = function(model, draw) {
                          list(fit = median(draw))
                        })

their_method <- new_method("their-method", "Their Method",
                           method = function(model, draw) {
                             list(fit = mean(draw))
                           })

```

Define methods to be used on the model. The function "new_method" has for arguments a name (for R), a pretty name, and the "method" named arg for the computation we want.

### what is in eval_functions

```{r}
his_loss <- new_metric("hisloss", "His loss function",
                        metric = function(model, out) {
                          return((model$mu - out$fit)^2)
})

her_loss <- new_metric("herloss", "Her loss function",
                        metric = function(model, out) {
                          return(abs(model$mu - out$fit))
                        })

```

Metric objects : shows how to compare model object and output of the method (method used on sim) object.

### what is in main

```{r}
setwd(dir)

# This is the main simulator file


library(simulator) # this file was created under simulator version 0.2.5

source("model_functions.R")
source("method_functions.R")
source("eval_functions.R")

## @knitr init

name_of_simulation <- "normal-mean-estimation-with-contamination"

## @knitr main

sim <- new_simulation(name = name_of_simulation,
                      label = "Mean estimation under contaminated normal") %>%
  generate_model(make_my_model, seed = 123,
                 n = 50,
                 prob = as.list(seq(0, 1, length = 6)),
                 vary_along = "prob") %>%
  simulate_from_model(nsim = 10) %>%
  run_method(list(my_method, their_method)) %>%
  evaluate(list(his_loss, her_loss))

## @knitr plots

plot_eval_by(sim, "hisloss", varying = "prob")

## @knitr tables

tabulate_eval(sim, "herloss", output_type = "markdown",
              format_args = list(digits = 1))

```

main calls the different files.

? Can plot_eval_by be used for different metrics at once ? ? Can tabulate_eval be used for different metrics at once ?
## Example: power curve of the equality t-test
Here is the copy-paste of the "main" file in the folder *equality_test*.


```{r}
library(simulator) # this file was created under simulator version 0.2.5

source("equality_test/model_functions.R")
source("equality_test/method_functions.R")
source("equality_test/eval_functions.R")

## @knitr init

name_of_simulation <- "normal-mean-test"

## @knitr main

sim <- new_simulation(name = name_of_simulation,
                      label = "Test of mean") %>%
  generate_model(make_my_model_normal, seed = 13,
                 n = 20,
                 mu2 = as.list(seq(0,10,by=0.5)),
                 mu1=0,
                 sig=5,
                 vary_along="mu2")                 %>%
  simulate_from_model(nsim = 1000) %>%
  run_method(list(t_test)) %>%
  evaluate(list(pval_loss))

## @knitr plots


## @knitr tables

tabulate_eval(sim, "p_value", output_type = "markdown",
              format_args = list(digits = 5))

plot_eval_by(sim, "p_value", varying = "mu2", main = "Power curve with mu1=0 and varying mu2")

```
ToDo :

- vary n
- vary both n and mu2
## Important functions :

-   new_model()
-   new_method()
-   new_metric()
-   new_simulation
-   generate_model
-   simulate_from_model
-   run_method
-   evaluate
-   plot_eval, plot_eval_by, tabulate_eval.

## Pros & Cons of simulator

This is not really a package that codes a method, but instead it proposes an architecture to store your codes, output simulations, results, etc.

::: columns
::: column
::: {.callout-note icon="false"}
### Pros

-   any model possible, if you can write it !
-   possible to iterate over parameter with pretty pipes
-   parallel possible, because you choose what you use
-   stores all results in the storage with increasing depth:

``` txt
files
└── name_of_model
    └── name_of_first_param_value
        └── name_of2nd_param_value ... model.Rdata out stores all sims
            └── r?.Rdata
```
:::
:::

::: column
::: {.callout-important icon="false"}
### Cons

-   Not an usual way to code in R, and not easy to explain. create the directory with the "create" function. Then, add the different functions, methods, models... in the corresponding files.
-   Mixes the code of the package/template with your own code.
-   stores all results in this neat way BUT if too many parameters, may exceed the depth allowed.
:::
:::
:::

# {SimEngine}

[{SimEngine}](https://cran.r-project.org/web/packages/SimEngine/) is an open-source R package for structuring, maintaining, running, and debugging statistical simulations on both local and cluster-based computing environments.
The paper describing the package is available [here](https://arxiv.org/pdf/2403.05698).


## Example

1. Create a simulation object `SimEngine::new_sim()`

```{r}
# library(SimEngine)
sim <- new_sim()
```

2. Create functions to generate data 
```{r}
create_data <- function(n) {
 return(rpois(n=n, lambda=20))
}
est_lambda <- function(dat, type) {
 if (type=="M") { return(mean(dat)) }
 if (type=="V") { return(var(dat)) }
 }
```

3. Simulation set-up

One run = one simulation replicate.
Features varying across simulation = simulation levels. Possible values = level values.
By default, SimEngine runs one simulation replicate for each combination of level value.

```{r}
sim %<>% set_levels(
 estimator = c("M", "V"),
 n = c(10, 100, 1000))
sim
```

4. Create a simulation script

i.e. generation, analysis and return results
```{r}
sim %<>% set_script(function() {
 dat <- create_data(n=L$n)
 lambda_hat <- est_lambda(dat=dat, type=L$estimator)
 return (list("lambda_hat"=lambda_hat))
 })
```

5. Configure and run the simulation
Using the `SimEngine::set_config()` it is possible to specify the number of replicates `num_sim`, the parallelization type `n_cores`, `parallel`,... and the required packages `packages` argument.

And we run the simulation with the `run` function.
```{r}
### Configuration
 sim %<>% set_config(
 num_sim = 100,
 packages = c("ggplot2", "stringr")
 )
### Run
sim %<>% run()
```

The package implements a `summarize` function to calculate usual summary statistics such as bias, variance, MSE.
```{r}
sim %>% summarize(
  list(stat="bias", name="bias_lambda", estimate="lambda_hat", truth=20),
  list(stat="mse", name="mse_lambda", estimate="lambda_hat", truth=20)
)
```
We can have information on individual simulation including runtime.
```{r}
head(sim$results)
```
It is possible to update simulation with more replicates or a new level. It keeps the old simulations and run only needed ones.
```{r}
sim %<>% set_config(num_sim = 200)
sim %<>% set_levels(
  estimator = c("M", "V"),
  n = c(10, 100, 1000, 10000)
)
sim %<>% update_sim()
```
## Parallelization

[A specific vignette](https://avi-kenny.github.io/SimEngine/articles/parallelization.html) is available and the introduction precises the terminology for parallel computing (node, core, task, job etc.).

There are two modes of parallelizing code : `local` or `cluster`. The first thing is to specify `set_config(parallel = TRUE)`. Local = split calculations on several cores of a single computer.
If the user’s computer has $n$ cores available, `{SimEngine}` will use n-1 cores by default.

cluster parallelization : function `run_on_cluster()`. To use the function, the user needs to break the code into three blocks : `first` (code run only once, set-up simulation object), `main` (a single call to `run()`) and `last` (the code will run after all simulation replicates have finished running and after SimEngine has automatically compiled the results into the simulation object.).

```{r}
run_on_cluster(
  first = {
    sim <- new_sim()
    create_data <- function(n) { return(rpois(n=n, lambda=20)) }
    est_lambda <- function(dat, type) {
      if (type=="M") { return(mean(dat)) }
      if (type=="V") { return(var(dat)) }
    }
    sim %<>% set_levels(estimator = c("M","V"), n = c(10,100,1000))
    sim %<>% set_script(function() {
      dat <- create_data(L$n)
      lambda_hat <- est_lambda(dat=dat, type=L$estimator)
      return(list("lambda_hat"=lambda_hat))
    })
    sim %<>% set_config(num_sim=100, n_cores=20)
  },
  main = {
    sim %<>% run()
  },
  last = {
    sim %>% summarize()
  },
  cluster_config = list(js="slurm")
)        
```
The `cluster_config` argument enables to specify options such as the choice of the scheduler.

Example on how to give instruction to the job scheduler is on the vignette.

Be caution : the number of cores cannot exceed the total number of simulation replicates

Function to update simulation on a CSS :`update_sim_on_cluster`. Difference is we do not need to create a new simulation config but load the existing simulation using `readRDS()` and use `set_config` or `set_levels` and `update_sim()` in the main block.

There is a vignette on advanced functionality such as complex results or simulation levels. It exists the `batch()` function to share data or objects between simulation replicates.

## Pros & Cons of {SimEngine}

::: columns
::: column
::: {.callout-note icon="false"}
### Pros

-   beginner friendly;
-   local and cluster-based computing environments;
-   well-written documentations and website with vignettes (with stat. formula of terminology for parallel computing)
-   information-sharing across simulation replicates (not tested)
-   automatic calculation of Monte Carlo error (not tested)
:::
:::

::: column
::: {.callout-important icon="false"}
### Cons

:::
:::
:::

## Simulation-based power calculation

A [specific vignette] is available on [the author's website](https://avi-kenny.github.io/SimEngine/articles/example_1.html)



## simChef

This document describes a simulation experiment using the `simChef` package in R, including data generation, method application, evaluation, and visualization.

A [specific vignette](https://yu-group.github.io/simChef/articles/simChef.html) is available on the author's website for more detailed instructions and examples on using thesimChefpackage.

## Setup

First, make sure to install and load the `simChef` package:

```{r}
# Install simChef package if not already installed
# devtools::install_github("Yu-Group/simChef")
library(simChef)
```

In simChef, a simulation experiment is divided into four components:

-   DGP(): the data-generating processes (DGPs) from which to generate data

-   Method(): the methods (or models) to fit on the data in the experiment

-   Evaluator(): the evaluation metrics used to evaluate the methods' performance

-   Visualizer(): the visualization procedures used to visualize outputs from the method fits or evaluation results (can be tables, plots, or even R Markdown snippets to display)

## Step 1: Define the Data-Generating Process, Methods, and Evaluation Functions

### Data-Generating Process

The following function generates pre- and post-treatment data:

```{r}
dgp_fun <- function(n, sd, mean_diff) {
  pre  <- rnorm(n, 0, sd)
  post <- pre + rnorm(n, mean_diff, sd)
  data_list <- list(
    pre = pre,
    post = post
  )
  return(data_list)
}
```

### Method

The following function applies a paired t-test to the data:

```{r}
method_fun <- function(pre, post) {
  test <- t.test(post, pre, paired = TRUE)
  return(test)
}
```

### Evaluation

The following function evaluates the power of the test:

```{r}
evaluation_fun <- function(fit_results){
  Power <- fit_results |> 
    dplyr::group_by(n, mean_diff, sd) |> 
    dplyr::summarize(Power = mean(p.value < 0.05)) 
 return(Power)
}
```

### Visualization

The following function creates a plot to visualize the power:

```{r}
power_plot_fun <- function(fit_results, eval_results){
  out_plot <- fit_results |> 
    dplyr::group_by(n, mean_diff, sd) |> 
    dplyr::summarize(Power = mean(p.value < 0.05)) |> 
    ggplot(aes(n, Power)) + 
    geom_col() + 
    facet_grid(rows = vars(sd), cols = vars(mean_diff)) + 
    theme_bw()
  return(out_plot)
}
```

## Step 2: Convert Functions into simChef Class Objects

```{r}
dgp <- create_dgp(
  .dgp_fun = dgp_fun, .name = "DGP"
)

method <- create_method(
  .method_fun = method_fun, .name = "T-test"
)

evaluation <- create_evaluator(
  .eval_fun = evaluation_fun , .name = 'P.value'
)

power_plot <- create_visualizer(
  .viz_fun = power_plot_fun, .name = 'Power plot'
)
```

## Step 3: Assemble the Simulation Experiment

```{r}
experiment <- create_experiment(name = "Example Experiment") %>%
  add_dgp(dgp) %>%
  add_method(method) %>%
  add_evaluator(evaluation) %>%
  add_visualizer(power_plot)

##For vary the simulation parameters
experiment <- experiment %>%
  add_vary_across(.dgp = "DGP",
                  n = c(100, 150, 200),
                  mean_diff = c(10, 20, 30),
                  sd = c(50, 100))

print(experiment)
```

## Step 4: Run the Experiment

```{r}
results <- run_experiment(experiment, n_reps = 100, save = TRUE)
results$fit_results %>% DT::datatable()
results$viz_results
```

## Pros & Cons of simChef

::: columns
::: column
::: {.callout-note icon="false"}
### Pros

-   Automated generation of an interactive R Markdown document (see init_docs() and render_docs() functions);
-   Beginner friendly;
-   Computing experimental replicates in parallel easily with future by adding plan(multisession, workers = n_workers) before run_experiment(experiment,...);
-   Flexibility of the return fitting results of the simulation (not necessarily the same outputs for all methods);
-   We can change the evaluation metrics and the visualization without re-fitting all the simulations by saving the fit_results tibble;
:::
:::

::: column
::: {.callout-important icon="false"}
### Cons

-   Likely not as fast/optimized as some alternatives;
-   Only save the simulation results computed from the evaluation functions. We cannot debug a strange simulation result;
:::
:::
:::
