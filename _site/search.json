[
  {
    "objectID": "simulator.html",
    "href": "simulator.html",
    "title": "Setting up simulations in R",
    "section": "",
    "text": "We explore several R packages for generate or structure simulations. Most statistical simulations studies includes different steps : generate data/ run one or several methods using simulated data / compare results.\nWe identified the following list of packages for data simulation:\n\n{simulator}: A framework for performing simulations such as those common in methodological statistics papers. The design principles of this package are described in greater depth in Bien, J. (2016) “The simulator: An Engine to Streamline Simulations,” which is available at doi:10.48550/arXiv.1607.00021.\n{simpr}: A general, ‘tidyverse’-friendly framework for simulation studies, design analysis, and power analysis. Specify data generation, define varying parameters, generate data, fit models, and tidy model results in a single pipeline, without needing loops or custom functions.\n{DeclareDesign}: Researchers can characterize and learn about the properties of research designs before implementation using ‘DeclareDesign’. Ex ante declaration and diagnosis of designs can help researchers clarify the strengths and limitations of their designs and to improve their properties, and can help readers evaluate a research strategy prior to implementation and without access to results. It can also make it easier for designs to be shared, replicated, and critiqued.\n{MonteCarlo}: Simplifies Monte Carlo simulation studies by automatically setting up loops to run over parameter grids and parallelising the Monte Carlo repetitions. It also generates LaTeX tables.\n{simChef}: The goal is to help you quickly cook up a fully-realized, high-quality, reproducible, and transparently-documented simulation study in a flexible, efficient, and low-code manner. It removes many of the administrative burdens of simulation design through:\n\nAn intuitive tidy grammar of data science simulations\nPowerful abstractions for distributed simulation processing backed by future\nAutomated generation of interactive R Markdown simulation documentation, situating results next to the workflows needed to reproduce them.\n\n{simEngine}: An open-source R package for structuring, maintaining, running, and debugging statistical simulations on both local and cluster-based computing environments. See full documentation at https://avi-kenny.github.io/SimEngine/."
  },
  {
    "objectID": "simulator.html#overview",
    "href": "simulator.html#overview",
    "title": "Setting up simulations in R",
    "section": "",
    "text": "We explore several R packages for generate or structure simulations. Most statistical simulations studies includes different steps : generate data/ run one or several methods using simulated data / compare results.\nWe identified the following list of packages for data simulation:\n\n{simulator}: A framework for performing simulations such as those common in methodological statistics papers. The design principles of this package are described in greater depth in Bien, J. (2016) “The simulator: An Engine to Streamline Simulations,” which is available at doi:10.48550/arXiv.1607.00021.\n{simpr}: A general, ‘tidyverse’-friendly framework for simulation studies, design analysis, and power analysis. Specify data generation, define varying parameters, generate data, fit models, and tidy model results in a single pipeline, without needing loops or custom functions.\n{DeclareDesign}: Researchers can characterize and learn about the properties of research designs before implementation using ‘DeclareDesign’. Ex ante declaration and diagnosis of designs can help researchers clarify the strengths and limitations of their designs and to improve their properties, and can help readers evaluate a research strategy prior to implementation and without access to results. It can also make it easier for designs to be shared, replicated, and critiqued.\n{MonteCarlo}: Simplifies Monte Carlo simulation studies by automatically setting up loops to run over parameter grids and parallelising the Monte Carlo repetitions. It also generates LaTeX tables.\n{simChef}: The goal is to help you quickly cook up a fully-realized, high-quality, reproducible, and transparently-documented simulation study in a flexible, efficient, and low-code manner. It removes many of the administrative burdens of simulation design through:\n\nAn intuitive tidy grammar of data science simulations\nPowerful abstractions for distributed simulation processing backed by future\nAutomated generation of interactive R Markdown simulation documentation, situating results next to the workflows needed to reproduce them.\n\n{simEngine}: An open-source R package for structuring, maintaining, running, and debugging statistical simulations on both local and cluster-based computing environments. See full documentation at https://avi-kenny.github.io/SimEngine/."
  },
  {
    "objectID": "simulator.html#how-to-choose",
    "href": "simulator.html#how-to-choose",
    "title": "Setting up simulations in R",
    "section": "How to choose?",
    "text": "How to choose?\nHere is a comparison of the different packages summarised in a table:\n\n\n\nName\nVersion\n#deps\n#rev deps\nLatest commit\nLatest release\nDoc\nOn CRAN?\nDevelopers\n\n\n\n\n{DeclareDesign}\n1.0.10\n2\n1\n2024-04-13\n2024-04-21\n\nYes\nGraeme Blair\n\n\n{MonteCarlo}\n1.0.6\n6\n0\n2019-01-31\n2019-01-31\n\nYes\nChristian Hendrik Leschinski\n\n\n{simChef}\n0.1.0\n22\n0\n2024-03-20\nNA\n\nNo\nTiffany Tang, James Duncan\n\n\n{simEngine}\n1.4.0\n6\n0\n2024-04-13\n2024-04-04\n\nYes\nAvi Kenny, Charles Wolock\n\n\n{simpr}\n0.2.6\n11\n0\n2024-07-16\n2023-04-26\n\nYes\nEthan Brown\n\n\n{simulator}\n0.2.5\n1\n0\n2023-02-02\n2023-02-04\n\nYes\nJacob Bien\n\n\n\nThis table shows that all packages are on CRAN, except for {simChef}. The latest release of {simChef} is not available, but the latest commit was in March 2024 so it is actively maintained. The number of dependencies is quite high for {simChef} and {simpr}. The number of reverse dependencies is low for all packages. The package {MonteCarlo} seams not to be maintained anymore.\nAlso, in terms of philosophy, the {DeclareDesign} package is dedicated to experimental design. It also makes it possible to simulate an experimental design of interest, in order to understand the properties of this design. As its goal is not to evaluate computational methods via simulations, it does not address the question of interest and we did not evaluate it further. The underlying experimental design principles are described in the companion book: https://book.declaredesign.org/.\nIn the following, we will therefore focus on the packages {simEngine}, {simChef}, {simpr}, and {simulator}. The next section describes the common simulation problem we will use to compare the packages. Then, we will show how to solve this problem with each package. Finally, we will compare the packages based on the code, the output, and the ease of use."
  },
  {
    "objectID": "simulator.html#a-common-simulation-problem-power-curve-for-test-calibration",
    "href": "simulator.html#a-common-simulation-problem-power-curve-for-test-calibration",
    "title": "Setting up simulations in R",
    "section": "A common simulation problem: power curve for test calibration",
    "text": "A common simulation problem: power curve for test calibration\nWe consider a common simulation problem: power curve estimation for hypothesis test calibration.\nWe consider a two-sample paired t-test simple example. The first sample is generated from a normal distribution with mean 0 and standard deviation sd. The second sample is generated from a normal distribution with mean mean_diff and standard deviation sd. The sample size is n. We want to estimate the power of the paired t-test for different values of mean_diff and ds. Specifically, we will use the following parameters:\n\nn: 100, 150, 200;\nmean_diff: 10, 20, 30;\nsd: 50, 100.\n\nWe will estimate the power by simulating the paired t-test for each combination of parameters. We will repeat the simulation 10 times for each combination of parameters.\nThe base R solution for this problem can look like this:\n\n## Set up parameters\nns &lt;- c(100L, 150L, 200L)\nmean_diffs &lt;- c(10, 20, 30)\nsds &lt;- c(50, 100)\nreps &lt;- 10L\n\n## Bring together into data frame\nresults_template &lt;- expand.grid(\n  n = ns, \n  mean_diff = mean_diffs, \n  sd = sds, \n  p.value = NA_real_\n)\nbase_r_sim &lt;- results_template[rep(1:nrow(results_template), each = reps), ]\n\n## Loop over rows of the data frame and calculate the p-value\nfor (i in 1:nrow(results_template)) {\n  params &lt;- base_r_sim[i,]\n  pre &lt;- rnorm(params$n, 0, params$sd)\n  post &lt;- pre + rnorm(params$n, params$mean_diff, params$sd)\n  base_r_sim$p.value[i] &lt;- t.test(pre, post)$p.value\n}\n\n## Display table output\nDT::datatable(base_r_sim)"
  },
  {
    "objectID": "simulator.html#simpr",
    "href": "simulator.html#simpr",
    "title": "Setting up simulations in R",
    "section": "{simpr}",
    "text": "{simpr}\nWhat is bad in the base R solution according to {simpr} authors:\n\nMost important pieces (data generating process, model specification, definitions, varying parameters) are hidden;\nWhat if there is an error?\nWhat about parallelization?\nIs this code sufficiently readable? Without the comments?\n\n\n{simpr} solution\n\nsimpr_tbl &lt;- specify(\n  pre  = ~ rnorm(n, 0, sd),\n  post = ~ pre + rnorm(n, mean_diff, sd)\n) |&gt; \n  define(n = ns, mean_diff = mean_diffs, sd = sds) |&gt; \n  generate(reps, .progress = TRUE) |&gt; \n  fit(t = ~ t.test(post, pre, paired = TRUE)) |&gt; \n  tidy_fits()\n\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:magrittr':\n\n    set_names\n\nDT::datatable(simpr_tbl)\n\n\n\n\nsimpr_tbl |&gt; \n  dplyr::group_by(n, mean_diff, sd) |&gt; \n  dplyr::summarize(Power = mean(p.value &lt; 0.05)) |&gt; \n  dplyr::ungroup() |&gt; \n  ggplot(aes(n, Power)) + \n  geom_col() + \n  facet_grid(rows = dplyr::vars(sd), cols = dplyr::vars(mean_diff)) + \n  theme_bw()\n\n`summarise()` has grouped output by 'n', 'mean_diff'. You can override using\nthe `.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\nPhilosophy\nThe {simpr} workflow, inspired by the {infer} package, distills a simulation study into five primary steps:\n\nspecify() your data-generating process;\ndefine() parameters that you want to systematically vary across your simulation design (e.g. n, effect size);\ngenerate() the simulation data;\nfit() models to your data (e.g. lm());\ntidy_fits() for consolidating results using broom::tidy(), such as computing power or Type I Error rates.\n\n\n\nReproducible workflows\n\nSame seed, same results;\nCan regenerate just a specific subset to see what happened in that particular dataset or fit;\nUseful in debugging and diagnosing unexpected results, etc.\n\n\nFiltering full simulation\n\nwithr::with_seed(500, {\n  specify(a = ~ runif(6)) |&gt; \n    generate(3) |&gt; \n    dplyr::filter(.sim_id == 3)\n})\n\nfull tibble\n--------------------------\n# A tibble: 1 × 3\n  .sim_id   rep sim             \n    &lt;int&gt; &lt;int&gt; &lt;list&gt;          \n1       3     3 &lt;tibble [6 × 1]&gt;\n\nsim[[1]]\n--------------------------\n# A tibble: 6 × 1\n      a\n  &lt;dbl&gt;\n1 0.371\n2 0.959\n3 0.633\n4 0.177\n5 0.803\n6 0.133\n\n\n\n\nSimulate subset only\n\nwithr::with_seed(500, {\n  specify(a = ~ runif(6)) |&gt; \n    generate(3, .sim_id == 3)\n})\n\nfull tibble\n--------------------------\n# A tibble: 1 × 3\n  .sim_id   rep sim             \n    &lt;int&gt; &lt;int&gt; &lt;list&gt;          \n1       3     3 &lt;tibble [6 × 1]&gt;\n\nsim[[1]]\n--------------------------\n# A tibble: 6 × 1\n      a\n  &lt;dbl&gt;\n1 0.371\n2 0.959\n3 0.633\n4 0.177\n5 0.803\n6 0.133\n\n\n\n\nBenchmarking\n\nbench::mark(\n  all = specify(a = ~ runif(6)) |&gt; \n    generate(1000) |&gt; \n    dplyr::filter(.sim_id == 1000),\n  subset = specify(a = ~ runif(6)) |&gt; \n    generate(1000, .sim_id == 1000),\n  check = FALSE, min_iterations = 10L, relative = TRUE\n)\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression   min median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 all         55.6   50.1       1        1        1   \n2 subset       1      1        52.5      1.28     1.27\n\n\n\n\n\nOther features\n\n\n\n\n\n\nData munging\n\n\n\nAdd per_sim() |&gt; after generate() in your simulation pipeline and then any tidyverse function that will apply to every simulation dataset:\n\nspecify(\n  pre  = ~ rnorm(n, 0, sd), \n  post = ~ pre + rnorm(n, mean_diff, sd)\n) |&gt; \n  define(n = ns, mean_diff = mean_diffs, sd = sds) |&gt; \n  generate(reps, .progress = TRUE) |&gt; \n  ## Apply tidyverse functions to every simulation dataset\n  per_sim() |&gt; \n  ## Mutate to add a range restriction\n  dplyr::mutate(dplyr::across(dplyr::everything(), dplyr::case_when(\n    pre &gt;  100 ~ 100,\n    pre &lt; -100 ~ -100,\n    .default   ~ pre\n  ))) |&gt; \n  fit(t = ~ t.test(post, pre, paired = TRUE)) |&gt; \n  tidy_fits()\n\n\n\n\n\n\n\n\n\nError handling\n\n\n\n\nCan change error handling to keep going with simulation, stop simulation, or to skip warnings;\nDebug and recovery options to enter into simulation during error.\n\n\n\n\n\n\n\n\n\nBuilt-in parallelization\n\n\n\nJust add\n\nlibrary(future)\nplan(multisession, workers = 6) # or however many cores are reasonable to use\n\nand your simulation pipeline (actually the generate() function) will run in parallel.\n\n\n\n\nPros & cons\n\n\n\n\n\n\n\n\nPros\n\n\n\n\ntidyverse friendly;\nbeginner friendly;\nReproducibility, error handling built in;\nGeneral-purpose, customizable and can handle arbitrary R code.\n\n\n\n\n\n\n\n\n\n\nCons\n\n\n\n\nLikely not as fast/optimized as some alternatives;\nNot as customizable/powerful as DeclareDesign;\nNot specifically set up for any particular application (no MC errors, plots, reports, specific models…)."
  },
  {
    "objectID": "simulator.html#simulator",
    "href": "simulator.html#simulator",
    "title": "Setting up simulations in R",
    "section": "{simulator}",
    "text": "{simulator}\nThis is a package on the CRAN. It is described in a 2016 paper by Jacob Bien. Last update on GitHub : last year (so, 2023).\n\nGetting started\nThe function create(), with a directory that does not exist, will create the directory with 5 files and 1 folder:\n\neval_functions.R: contains metrics to be evaluated;\nfiles/: directory to store results;\nmain.R: main code to run;\nmethod_functions.R: methods to run;\nmodel_functions.R: define the models;\nwriteup.Rmd.\n\n\nsimulator_dir &lt;- \"./sims_simulator\"\nif (!file.exists(simulator_dir))\n  create(simulator_dir)\n\nNew simulation template created!  Go to ./sims_simulator/main.R to get started.\n\n\n\nwithr::with_dir(simulator_dir, {\n  list.files()\n})\n\n[1] \"eval_functions.R\"   \"main.R\"             \"method_functions.R\"\n[4] \"model_functions.R\"  \"writeup.Rmd\"       \n\n\n\nOn a typical project, one starts by defining a model in model_functions.R, one or two methods in method_functions.R, and a few metrics in eval_functions.R, and then one runs the code in main.R. After looking at some of the results, one might add an additional model or method or metric. One then returns to main.R, adds some additional lines specifying that the additional components should be run as well and looks at some more results.\nThe simplest way to look at results is by using the plot functions plot_eval(), plot_evals() and plot_evals_by(). In situations where you wish to investigate results more deeply than just looking at aggregated plots, one can use the functions model(), draws(), output(), and evals() to get at all objects generated through the course of the simulation.\n\nThe create() function also creates the template in the different files:\n\nContent of model_functions.R\n\nmake_my_model &lt;- function(n, prob) {\n  new_model(\n    name = \"contaminated-normal\", \n    label = sprintf(\"Contaminated normal (n = %s, prob = %s)\", n, prob), \n    params = list(n = n, mu = 2, prob = prob), \n    simulate = function(n, mu, prob, nsim) {\n      # this function must return a list of length nsim\n      contam &lt;- runif(n * nsim) &lt; prob\n      x &lt;- matrix(rep(NA, n * nsim), n, nsim)\n      x[contam] &lt;- rexp(sum(contam))\n      x[!contam] &lt;- rnorm(sum(!contam))\n      x &lt;- mu + x # true mean is mu\n      return(split(x, col(x))) # make each col its own list element\n    }\n  )\n}\n\nDefine a model from its different components with new_model():\n\nname;\nlabel: what will be printed in the tables later probably?\nparam: a list of different parameters for the model;\nsimulate: a function of the parameters that returns nsim simulations.\n\n\n\nContent of method_functions.R\n\nmy_method &lt;- new_method(\n  name = \"my-method\", \n  label = \"My Method\", \n  method = function(model, draw) {\n    list(fit = median(draw))\n  }\n)\n\ntheir_method &lt;- new_method(\n  name = \"their-method\", \n  label = \"Their Method\",\n  method = function(model, draw) {\n    list(fit = mean(draw))\n  }\n)\n\nDefine methods to be used on the model. The function new_method() has for arguments a name (for R) name, a pretty name label, and the method named arg for the computation we want.\n\n\nContent of eval_functions.R\n\nhis_loss &lt;- new_metric(\n  name = \"hisloss\", \n  label = \"His loss function\",\n  metric = function(model, out) {\n    return((model$mu - out$fit)^2)\n  }\n)\n\nher_loss &lt;- new_metric(\n  name = \"herloss\", \n  label = \"Her loss function\",\n  metric = function(model, out) {\n    return(abs(model$mu - out$fit))\n  }\n)\n\nMetric objects: shows how to compare model object and output of the method (method used on sim) object.\n\n\nContent of main.R\n\nsetwd(simulator_dir)\n\nsource(\"model_functions.R\")\nsource(\"method_functions.R\")\nsource(\"eval_functions.R\")\n\n## @knitr init\n\nname_of_simulation &lt;- \"normal-mean-estimation-with-contamination\"\n\n## @knitr main\n\nsim &lt;- new_simulation(\n  name = name_of_simulation,\n  label = \"Mean estimation under contaminated normal\"\n) %&gt;%\n  generate_model(\n    make_model = make_my_model, \n    seed = 123,\n    n = 50,\n    prob = as.list(seq(0, 1, length = 6)),\n    vary_along = \"prob\"\n  ) %&gt;%\n  simulate_from_model(nsim = 10) %&gt;%\n  run_method(list(my_method, their_method)) %&gt;%\n  evaluate(list(his_loss, her_loss))\n\n## @knitr plots\n\nplot_eval_by(sim = sim, metric_name = \"hisloss\", varying = \"prob\")\n\n## @knitr tables\n\ntabulate_eval(\n  object = sim, \n  metric_name = \"herloss\", \n  output_type = \"markdown\",\n  format_args = list(digits = 1)\n)\n\nThe main.R script calls the different files.\nCan plot_eval_by() be used for different metrics at once? Can tabulate_eval() be used for different metrics at once?\n\n\n\nExample: power curve\nHere is the content of the main.R file in the folder simulator_equality_test/.\n\nlibrary(simulator) # this file was created under simulator version 0.2.5\n\nsource(\"simulator_equality_test/model_functions.R\")\nsource(\"simulator_equality_test/method_functions.R\")\nsource(\"simulator_equality_test/eval_functions.R\")\n\n## @knitr init\n\nname_of_simulation &lt;- \"normal-mean-test\"\n\n## @knitr main\n\nsuppressMessages(\n  \n  sim &lt;- new_simulation(\n    name = name_of_simulation, \n    label = \"Test of mean\"\n  ) |&gt; \n    generate_model(\n      make_model = make_my_model_normal, \n      seed = 13, \n      n = 20, \n      mu2 = as.list(seq(0, 10, by = 0.5)), \n      mu1 = 0, \n      sig = 5, \n      vary_along = \"mu2\"\n    ) |&gt; \n    simulate_from_model(nsim = 1000) |&gt; \n    run_method(list(t_test)) |&gt; \n    evaluate(list(pval_loss))\n\n)\n\n..Created model and saved in normal/mu1_0/mu2_0/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_0.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_1/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_1.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_2/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_2.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_3/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_3.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_4/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_4.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_5.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_6/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_6.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_7/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_7.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_8/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_8.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_9/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_9.5/n_20/sig_5/model.Rdata\n..Created model and saved in normal/mu1_0/mu2_10/n_20/sig_5/model.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_0/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_0.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_1/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_1.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_2/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_2.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_3/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_3.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_4/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_4.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_5.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_6/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_6.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_7/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_7.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_8/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_8.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_9/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0.01 sec and saved in normal/mu1_0/mu2_9.5/n_20/sig_5/r1.Rdata\n..Simulated 1000 draws in 0 sec and saved in normal/mu1_0/mu2_10/n_20/sig_5/r1.Rdata\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Performed Mean equality test in 0 seconds (on average over 1000 sims)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n..Evaluated Mean equality test in terms of pval&lt;0.05, Computing time (sec)\n\n## @knitr tables\n\ntabulate_eval(\n  sim, \n  metric_name = \"p_value\", \n  output_type = \"markdown\",\n  format_args = list(digits = 5)\n)\n\n&lt;!-- generated by simulator on Thu Aug 22 16:33:15 2024. --&gt;\n\n\n\nA comparison of Mean pval&lt;0.05 (averaged over 1000 replicates).\n\n\n\nMean equality test\n\n\n\n\nnormal\n0.055 (0.0072130)\n\n\nnormal\n0.067 (0.0079103)\n\n\nnormal\n0.100 (0.0094916)\n\n\nnormal\n0.143 (0.0110758)\n\n\nnormal\n0.226 (0.0132325)\n\n\nnormal\n0.327 (0.0148422)\n\n\nnormal\n0.441 (0.0157088)\n\n\nnormal\n0.559 (0.0157088)\n\n\nnormal\n0.671 (0.0148654)\n\n\nnormal\n0.783 (0.0130415)\n\n\nnormal\n0.860 (0.0109782)\n\n\nnormal\n0.907 (0.0091889)\n\n\nnormal\n0.955 (0.0065588)\n\n\nnormal\n0.982 (0.0042064)\n\n\nnormal\n0.992 (0.0028185)\n\n\nnormal\n0.995 (0.0022316)\n\n\nnormal\n0.999 (0.0010000)\n\n\nnormal\n0.999 (0.0010000)\n\n\nnormal\n0.999 (0.0010000)\n\n\nnormal\n1.000 (0.0000000)\n\n\nnormal\n1.000 (0.0000000)\n\n\n\n\n## @knitr plots\n\nplot_eval_by(\n  sim, \n  metric_name = \"p_value\", \n  varying = \"mu2\", \n  main = \"Power curve with mu1=0 and varying mu2\"\n)\n\n\n\n\n\n\n\n\nToDo :\n\nvary n\nvary both n and mu2\n\n\n\nImportant functions\n\nnew_model();\nnew_method();\nnew_metric();\nnew_simulation();\ngenerate_model();\nsimulate_from_model();\nrun_method();\nevaluate();\nplot_eval(), plot_eval_by(), tabulate_eval()."
  },
  {
    "objectID": "simulator.html#pros-cons-1",
    "href": "simulator.html#pros-cons-1",
    "title": "Setting up simulations in R",
    "section": "Pros & cons",
    "text": "Pros & cons\nThis is not really a package that codes a method, but instead it proposes an architecture to store your codes, output simulations, results, etc.\n\n\n\n\n\n\n\n\nPros\n\n\n\n\nany model possible, if you can write it;\npossible to iterate over parameter with pretty pipes;\nparallel possible, because you choose what you use;\nstores all results in the storage with increasing depth:\n\nfiles\n└── name_of_model\n    └── name_of_first_param_value\n        └── name_of2nd_param_value ... model.Rdata out stores all sims\n            └── r?.Rdata\n\n\n\n\n\n\n\n\n\nCons\n\n\n\n\nNot an usual way to code in R, and not easy to explain. Create the directory with the create() function. Then, add the different functions, methods, models, etc., in the corresponding files;\nMixes the code of the package/template with your own code;\nStores all results in this neat way BUT if too many parameters, may exceed the depth allowed."
  },
  {
    "objectID": "simulator.html#simengine",
    "href": "simulator.html#simengine",
    "title": "Setting up simulations in R",
    "section": "{simEngine}",
    "text": "{simEngine}\n{simEngine} is an open-source R package for structuring, maintaining, running, and debugging statistical simulations on both local and cluster-based computing environments. The paper describing the package is available here.\n\nExample\n\nCreate a simulation object SimEngine::new_sim()\n\n\nsim &lt;- new_sim()\n\n\nCreate functions to generate data\n\n\ncreate_data &lt;- function(n) {\n  return(rpois(n = n, lambda = 20))\n}\n\nest_lambda &lt;- function(dat, type) {\n  if (type==\"M\") {\n    return(mean(dat))\n  }\n  if (type==\"V\") {\n    return(var(dat))\n  }\n}\n\n\nSimulation set-up\n\nOne run = one simulation replicate. Features varying across simulation = simulation levels. Possible values = level values. By default, {simEngine} runs one simulation replicate for each combination of level value.\n\nsim &lt;- sim |&gt; \n  set_levels(\n    estimator = c(\"M\", \"V\"),\n    n = c(10, 100, 1000)\n  )\nsim\n\nSimEngine: simulation object (class \"sim_obj\")\n----------------------------------------------\nConfiguration: \n    num_sim: 1\n    parallel: FALSE\n    n_cores: NA\n    packages: NULL\n    stop_at_error: FALSE\n    seed: 528361534\n    progress_bar: TRUE\n    batch_levels: NA\n    return_batch_id: FALSE\nLevels: \n    estimator: c(\"M\", \"V\")\n    n: c(10, 100, 1000)\nState: pre run\n\n\n\nCreate a simulation script\n\ni.e. generation, analysis and return results\n\nsim &lt;- sim |&gt; \n  set_script(function() {\n    dat &lt;- create_data(n = L$n)\n    lambda_hat &lt;- est_lambda(dat = dat, type = L$estimator)\n    return(list(\"lambda_hat\" = lambda_hat))\n  })\n\n\nConfigure and run the simulation\n\nUsing the SimEngine::set_config() it is possible to specify the number of replicates num_sim, the parallelization type n_cores, parallel, … and the required packages packages argument.\nAnd we run the simulation with the SimEngine::run() function.\n\n### Configuration\nsim &lt;- sim |&gt; \n  set_config(\n    num_sim = 100,\n    packages = c(\"ggplot2\", \"stringr\")\n  )\n\n### Run\nsim &lt;- run(sim)\n\nDone. No errors or warnings detected.\n\n\nThe package implements a SimEngine::summarize() function to calculate usual summary statistics such as bias, variance, MSE.\n\nsim |&gt; \n  summarize(\n    list(\n      stat = \"bias\",\n      name = \"bias_lambda\",\n      estimate = \"lambda_hat\",\n      truth = 20\n    ), \n    list(\n      stat = \"mse\",\n      name = \"mse_lambda\",\n      estimate = \"lambda_hat\",\n      truth = 20\n    )\n  )\n\n  level_id estimator    n n_reps bias_lambda  mse_lambda\n1        1         M   10    100  -0.0930000  1.75850000\n2        2         V   10    100   0.3353333 96.23611111\n3        3         M  100    100  -0.0877000  0.20287100\n4        4         V  100    100  -0.3656687  9.04930168\n5        5         M 1000    100   0.0198000  0.02163858\n6        6         V 1000    100   0.1130303  0.75417061\n\n\nWe can have information on individual simulation including runtime.\n\nhead(sim$results)\n\n  sim_uid level_id rep_id estimator  n      runtime lambda_hat\n1       1        1      1         M 10 0.0004179478       20.3\n2       7        1      2         M 10 0.0002481937       20.4\n3       8        1      3         M 10 0.0002350807       19.8\n4       9        1      4         M 10 0.0002431870       19.1\n5      10        1      5         M 10 0.0002410412       19.8\n6      11        1      6         M 10 0.0002489090       20.9\n\n\nIt is possible to update simulation with more replicates or a new level. It keeps the old simulations and run only needed ones.\n\nsim &lt;- sim |&gt; \n  set_config(num_sim = 200) |&gt; \n  set_levels(\n    estimator = c(\"M\", \"V\"),\n    n = c(10, 100, 1000, 10000)\n  ) |&gt; \n  update_sim()\n\nDone. No errors or warnings detected.\n\n\n\n\nParallelization\nA specific vignette is available and the introduction precises the terminology for parallel computing (node, core, task, job, etc.). There are two modes of parallelizing code: local or cluster. The first thing is to specify set_config(parallel = TRUE).\n\nLocal: split calculations on several cores of a single computer. If the user’s computer has \\(n\\) cores available, {simEngine} will use \\(n-1\\) cores by default.\nCluster: function run_on_cluster(). To use the function, the user needs to break the code into three blocks : first (code run only once, set-up simulation object), main (a single call to run()) and last (the code will run after all simulation replicates have finished running and after SimEngine has automatically compiled the results into the simulation object.).\n\n\nrun_on_cluster(\n  first = {\n    create_data &lt;- function(n) {\n      return(rpois(n = n, lambda = 20))\n    }\n    est_lambda &lt;- function(dat, type) {\n      if (type == \"M\") {\n        return(mean(dat))\n      }\n      if (type == \"V\") {\n        return(var(dat))\n      }\n    }\n    sim &lt;- new_sim() |&gt; \n      set_levels(estimator = c(\"M\", \"V\"), n = c(10, 100, 1000)) |&gt; \n      set_script(function() {\n        dat &lt;- create_data(L$n)\n        lambda_hat &lt;- est_lambda(dat = dat, type = L$estimator)\n        return(list(\"lambda_hat\" = lambda_hat))\n      }) |&gt; \n      set_config(num_sim = 100, n_cores = 20)\n  },\n  main = {\n    sim &lt;- run(sim)\n  },\n  last = {\n    sim &lt;- summarize(sim)\n  },\n  cluster_config = list(js = \"slurm\")\n)        \n\nDone. No errors or warnings detected.\n\n\nThe cluster_config argument enables to specify options such as the choice of the scheduler.\nExample on how to give instruction to the job scheduler is on the vignette.\nBe caution: the number of cores cannot exceed the total number of simulation replicates.\nFunction to update simulation on a CSS: update_sim_on_cluster(). Difference is we do not need to create a new simulation config but load the existing simulation using readRDS() and use set_config() or set_levels() and update_sim() in the main block.\nThere is a vignette on advanced functionality such as complex results or simulation levels. It exists the batch() function to share data or objects between simulation replicates."
  },
  {
    "objectID": "simulator.html#pros-cons-2",
    "href": "simulator.html#pros-cons-2",
    "title": "Setting up simulations in R",
    "section": "Pros & cons",
    "text": "Pros & cons\n\n\n\n\n\n\n\n\nPros\n\n\n\n\nbeginner friendly;\nlocal and cluster-based computing environments;\nwell-written documentations and website with vignettes (with stat. formula of terminology for parallel computing);\ninformation-sharing across simulation replicates (not tested);\nautomatic calculation of Monte Carlo error (not tested).\n\n\n\n\n\n\n\n\n\n\nCons\n\n\n\n\n\n\n\n\n\nSimulation-based power calculation\nA [specific vignette] is available on the author’s website."
  },
  {
    "objectID": "simulator.html#simchef",
    "href": "simulator.html#simchef",
    "title": "Setting up simulations in R",
    "section": "{simChef}",
    "text": "{simChef}\nThis document describes a simulation experiment using the {simChef} package in R, including data generation, method application, evaluation, and visualization.\nA specific vignette is available on the author’s website for more detailed instructions and examples on using the {simChef} package.\n\nSetup\nThe {simChef} package is not on CRAN and must therefore be installed from GitHub using the {remotes} package as follows:\n\nremotes::install_github(\"Yu-Group/simChef\")\n\nIn {simChef}, a simulation experiment is divided into four components:\n\nDGP(): the data-generating processes (DGPs) from which to generate data;\nMethod(): the methods (or models) to fit on the data in the experiment;\nEvaluator(): the evaluation metrics used to evaluate the methods’ performance;\nVisualizer(): the visualization procedures used to visualize outputs from the method fits or evaluation results (can be tables, plots, or even R Markdown snippets to display).\n\n\n\nStep 1: Define the Data-Generating Process, Methods, and Evaluation Functions\n\nData-Generating Process\nThe following function generates pre- and post-treatment data:\n\ndgp_fun &lt;- function(n, sd, mean_diff) {\n  pre  &lt;- rnorm(n, 0, sd)\n  post &lt;- pre + rnorm(n, mean_diff, sd)\n  list(pre = pre, post = post)\n}\n\n\n\nMethod\nThe following function applies a paired t-test to the data:\n\nmethod_fun &lt;- function(pre, post) {\n  t.test(post, pre, paired = TRUE)\n}\n\n\n\nEvaluation\nThe following function evaluates the power of the test:\n\nevaluation_fun &lt;- function(fit_results) {\n  Power &lt;- fit_results |&gt; \n    dplyr::group_by(n, mean_diff, sd) |&gt; \n    dplyr::summarize(Power = mean(p.value &lt; 0.05))\n}\n\n\n\nVisualization\nThe following function creates a plot to visualize the power:\n\npower_plot_fun &lt;- function(fit_results, eval_results) {\n  fit_results |&gt; \n    dplyr::group_by(n, mean_diff, sd) |&gt; \n    dplyr::summarize(Power = mean(p.value &lt; 0.05)) |&gt; \n    ggplot(aes(n, Power)) + \n    geom_col() + \n    facet_grid(rows = dplyr::vars(sd), cols = dplyr::vars(mean_diff)) + \n    theme_bw()\n}\n\n\n\n\nStep 2: Convert Functions into {simChef} Class Objects\n\ndgp &lt;- create_dgp(\n  .dgp_fun = dgp_fun, .name = \"DGP\"\n)\n\nmethod &lt;- create_method(\n  .method_fun = method_fun, .name = \"T-test\"\n)\n\nevaluation &lt;- create_evaluator(\n  .eval_fun = evaluation_fun , .name = 'P.value'\n)\n\npower_plot &lt;- create_visualizer(\n  .viz_fun = power_plot_fun, .name = 'Power plot'\n)\n\n\n\nStep 3: Assemble the Simulation Experiment\n\nexperiment &lt;- create_experiment(name = \"Example Experiment\") |&gt; \n  add_dgp(dgp) |&gt; \n  add_method(method) |&gt; \n  add_evaluator(evaluation) |&gt; \n  add_visualizer(power_plot)\n\n## Define the grid of simulation parameters\nexperiment &lt;- experiment |&gt; \n  add_vary_across(.dgp = \"DGP\", n = ns, mean_diff = mean_diffs, sd = sds)\n\nprint(experiment)\n\nExperiment Name: Example Experiment \n   Saved results at: results/Example Experiment \n   DGPs: DGP \n   Methods: T-test \n   Evaluators: P.value \n   Visualizers: Power plot \n   Vary Across: \n      DGP: DGP \n         n:  int [1:3] 100 150 200\n         mean_diff:  num [1:3] 10 20 30\n         sd:  num [1:2] 50 100\n\n\n\n\nStep 4: Run the Experiment\n\nresults &lt;- run_experiment(experiment, n_reps = reps, save = TRUE)\n\nFitting Example Experiment...\nSaving fit results...\nFit results saved | time taken: 0.011918 seconds\n10 reps completed (totals: 10/10) | time taken: 0.408452 minutes\n==============================\nEvaluating Example Experiment...\n`summarise()` has grouped output by 'n', 'mean_diff'. You can override using the `.groups` argument.\nEvaluation completed | time taken: 0.000100 minutes\nSaving eval results...\nEval results saved | time taken: 0.033090 seconds\n==============================\nVisualizing Example Experiment...\n`summarise()` has grouped output by 'n', 'mean_diff'. You can override using the `.groups` argument.\nVisualization completed | time taken: 0.000151 minutes\nSaving viz results...\nViz results saved | time taken: 0.053092 seconds\n==============================\n\nDT::datatable(results$fit_results)\n\n\n\n\nresults$viz_results\n\n$`Power plot`"
  },
  {
    "objectID": "simulator.html#pros-cons-3",
    "href": "simulator.html#pros-cons-3",
    "title": "Setting up simulations in R",
    "section": "Pros & cons",
    "text": "Pros & cons\n\n\n\n\n\n\n\n\nPros\n\n\n\n\nAutomated generation of an interactive R Markdown document (see init_docs() and render_docs() functions);\nBeginner friendly;\nComputing experimental replicates in parallel easily with future by adding plan(multisession, workers = n_workers) before run_experiment(experiment, ...);\nFlexibility of the return fitting results of the simulation (not necessarily the same outputs for all methods);\nWe can change the evaluation metrics and the visualization without re-fitting all the simulations by saving the fit_results tibble.\n\n\n\n\n\n\n\n\n\n\nCons\n\n\n\n\nLikely not as fast/optimized as some alternatives;\nOnly save the simulation results computed from the evaluation functions. We cannot debug a strange simulation result."
  },
  {
    "objectID": "simulator_equality_test/writeup.html",
    "href": "simulator_equality_test/writeup.html",
    "title": "My Simulation",
    "section": "",
    "text": "This is a knitr report generated by the simulator to describe your simulation. Knitting this file will rerun the simulation if any of the code files have been modified since the simulation object was last created."
  },
  {
    "objectID": "simulator_equality_test/writeup.html#models",
    "href": "simulator_equality_test/writeup.html#models",
    "title": "My Simulation",
    "section": "Models",
    "text": "Models\n\nmake_my_model_normal &lt;- function(n, mu1, mu2, sig) {\n  new_model(\n    name = \"normal\", \n    label = sprintf(\"normal\"), \n    params = list(n = n, mu1 = mu1, mu2 = mu2, sig = sig),\n    simulate = function(n, mu1,mu2, sig, nsim) {\n      # this function must return a list of length nsim\n      x1 &lt;- mu1 + sig * matrix(rnorm(nsim * n), n, nsim)\n      x2 &lt;- mu2 + sig * matrix(rnorm(nsim * n), n, nsim)\n      li1 &lt;- split(x1, col(x1))\n      li2 &lt;- split(x2, col(x2))\n      lapply(1:nsim, function(i) {\n        cbind(li1[[i]], li2[[i]])\n      })\n    }\n  )\n}"
  },
  {
    "objectID": "simulator_equality_test/writeup.html#methods",
    "href": "simulator_equality_test/writeup.html#methods",
    "title": "My Simulation",
    "section": "Methods",
    "text": "Methods\n\nt_test &lt;- new_method(\n  name = \"t-test\", \n  label = \"Mean equality test\",\n  method = function(model, draw) {\n    list(pvalue = t.test(draw[, 1], draw[, 2], paired = TRUE)$p.value)\n  }\n)"
  },
  {
    "objectID": "simulator_equality_test/writeup.html#metrics",
    "href": "simulator_equality_test/writeup.html#metrics",
    "title": "My Simulation",
    "section": "Metrics",
    "text": "Metrics\n\npval_loss &lt;- new_metric(\n  name = \"p_value\", \n  label = \"pval&lt;0.05\",\n  metric = function(model, out) {\n    mean(out$pvalue &lt; 0.05)\n  }\n)"
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "Instructions pour le dépot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2024.git\nAvec clés SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2024.git\n\n\n(Lien vers une doc complète)[https://docs.github.com/fr/get-started/getting-started-with-git/managing-remote-repositories]."
  },
  {
    "objectID": "instructions.html#cloner-le-dépôt-git-du-bootcamp",
    "href": "instructions.html#cloner-le-dépôt-git-du-bootcamp",
    "title": "Instructions pour le dépot sur le site web",
    "section": "",
    "text": "Protocole https :\ngit clone https://github.com/StateOfTheR/finistR2024.git\nAvec clés SSH : git clone git@github.com:git@github.com:StateOfTheR/finistR2024.git\n\n\n(Lien vers une doc complète)[https://docs.github.com/fr/get-started/getting-started-with-git/managing-remote-repositories]."
  },
  {
    "objectID": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "href": "instructions.html#processus-de-mise-en-commun-des-ateliers",
    "title": "Instructions pour le dépot sur le site web",
    "section": "Processus de mise en commun des ateliers",
    "text": "Processus de mise en commun des ateliers\n\nCréer une branche propre à l’atelier nommée explicitement mon_nom_parlant et basculer dessus\n\ngit checkout -b mon_nom_parlant\n\nCréer un fichier Rmarkdown de restitution de votre atelier fichier.Rmd dans votre branche\n\ngit add fichier.Rmd\ngit commit -m \"restitution atelier\"\n\nPousser vos modifications sur le serveur distant\n\ngit  push --set-upstream origin mon_nom_parlant ou\ngit  push\n\nFaire une pull request (PR) sur github\nindiquer dans le message de la PR la liste des packages ou autres besoins\nQuand la PR passe les tests, demander le merge.\ncorriger les erreurs éventuelles dans la compilation du Rmarkdown\nles admins peuvent avoir à mettre à jour l’image docker"
  },
  {
    "objectID": "instructions.html#détails-du-fonctionnement",
    "href": "instructions.html#détails-du-fonctionnement",
    "title": "Instructions pour le dépot sur le site web",
    "section": "Détails du fonctionnement",
    "text": "Détails du fonctionnement\n\nLe docker\n(Lien vers la fiche pense-bête)[https://www.docker.com/sites/default/files/d8/2019-09/docker-cheat-sheet.pdf]\nPour créer des images Docker en local sur sa machine, voici une liste de commandes utiles\n\nPour construire une image docker, il faut créer un fichier Dockerfile qui contient la recette du Docker. Pour ce site le ficher Dockerfile a la forme suivante\n\n\n\n\nFROM rocker/geospatial:4.4\nRUN export DEBIAN_FRONTEND=noninteractive; apt-get -y update \\\n && apt-get install -y pandoc \\\n    pandoc-citeproc\nRUN R -e \"install.packages('remotes')\"\nRUN R -e \"install.packages('microbenchmark')\"\nRUN R -e \"install.packages('purrr')\" # map function\nENV R_CRAN_WEB=\"https://cran.rstudio.com/\"\nRUN R -e \"install.packages('cowplot')\" # GET function\nRUN R -e \"install.packages('torch')\"\nRUN R -e \"torch::install_torch(type = 'cpu')\"\nRUN R -e \"install.packages('PLNmodels')\"\nRUN R -e \"install.packages('torchvision')\"\n\nRUN apt-get update \\\n && apt-get install -y --no-install-recommends \\\n  mercurial gdal-bin libgdal-dev gsl-bin libgsl-dev \\\n  libc6-i386\n\nRUN R -e \"install.packages('reticulate')\"\nRUN R -e \"install.packages(c('inlabru', 'lme4', 'ggpolypath', 'RColorBrewer', 'geoR'))\"\nRUN R -e \"install.packages(c('poissonreg'))\"\nRUN apt-get install -y --no-install-recommends unzip python3-pip dvipng pandoc wget git make python3-venv && \\\n    pip3 install jupyter jupyter-cache flatlatex matplotlib && \\\n    apt-get --purge -y remove texlive.\\*-doc$ && \\\n    apt-get clean\n\n\npuis demander la construction de l’image à l’aide de la commande\n\n docker build -t nom_depot_dockerhub/nom_du_repo:version  . ## avec un nom\n\net enfin pousser sur Dockerhub\n\n docker push nom_depot_dockerhub/nom_du_repo:version\n\n\n\nLes actions\nDans les action de Github, on peut spécifier un container docker à utiliser, c’est ce que fait la ligne container du fichier d’action suivant, utiliser pour créer ce site web\n\n\nname: website\non:\n  push:\n    branches:\n      - main\n\njobs:\n  build:\n    name: Build website with rmarkdown\n    runs-on: ubuntu-latest\n    container: stateofther/r-finistr2024:0.1\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n      - name: Additional Packages\n        run: Rscript -e \"install.packages(c('tictoc'))\"\n      - name: Generate slides\n        run: \"quarto render\"\n      - name: GitHub Pages action\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\n          publish_dir: ./_site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "",
    "text": "L’atelier Finist’R 2024 – ou bootcamp R du groupe State Of The R s’est déroulé à la station biologique de Roscoff du 19 au 23 août 2024.\nStateoftheR est un réseau du département MathNum INRAE.\n\n\nIl s’agit de la huitième édition de l’atelier Finist’R. Cet atelier réunit annuellement un groupe de chercheurs, ingénieurs, doctorants, tous utilisateurs avancés de R et développeurs de paquets pour explorer les dernières fonctionnalités du logiciel et les nouvelles pratiques de développement. A l’issue de l’atelier le collectif produit une synthèse de cette veille logiciel de manière à progresser collectivement dans l’utilisation du logiciel mais surtout dans la production d’outils statistiques à destination de la communauté.\nLe résultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#où-quand",
    "href": "index.html#où-quand",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "",
    "text": "L’atelier Finist’R 2024 – ou bootcamp R du groupe State Of The R s’est déroulé à la station biologique de Roscoff du 19 au 23 août 2024.\nStateoftheR est un réseau du département MathNum INRAE.\n\n\nIl s’agit de la huitième édition de l’atelier Finist’R. Cet atelier réunit annuellement un groupe de chercheurs, ingénieurs, doctorants, tous utilisateurs avancés de R et développeurs de paquets pour explorer les dernières fonctionnalités du logiciel et les nouvelles pratiques de développement. A l’issue de l’atelier le collectif produit une synthèse de cette veille logiciel de manière à progresser collectivement dans l’utilisation du logiciel mais surtout dans la production d’outils statistiques à destination de la communauté.\nLe résultat de cette semaine est disponible sur cette page"
  },
  {
    "objectID": "index.html#participants",
    "href": "index.html#participants",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "Participants",
    "text": "Participants\nBaptiste Alglave, Emré Anakok, Julie Aubert, Pierre Barbillon, Julien Chiquet, Lucia Clarotto, Caroline Cognot, Annaïg De Walsche, Sophie Donnet, Marie-Pierre Etienne, Armand Favrot, Hugo Gangloff, Pierre Gloaguen, Adeline Leclercq Samson, Tristan Mary-Huard, Cédric Midoux, Pierre Neuvial, Aymeric Stamm, Florian Teste, François Victor, Emily Walker."
  },
  {
    "objectID": "index.html#soutien",
    "href": "index.html#soutien",
    "title": "FinistR : bootcamp R à Roscoff",
    "section": "Soutien",
    "text": "Soutien"
  },
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "Ateliers Finist’R 2024",
    "section": "",
    "text": "Ateliers Finist’R 2024\n\n\n\nwebsite\n\n\nL’atelier Finist’R 2024 – ou bootcamp R s’est déroulé à la station biologique de Roscoff du 19 au 23 août 2024.\nIl s’agit de la huitième édition de l’atelier Finist’R. Cet atelier réunit annuellement un groupe de chercheurs, ingénieurs, doctorants, tous utilisateurs avancés de R et dévelopeurs de paquets pour explorer les dernières fonctionalités du logiciel et les nouvelles pratiques de développement. A l’issu de l’atelier le collectif produit une synthèse de cette veille logiciel de manière à progresser collectivement dans l’utilisation du logiciel mais surtout dans la production d’outils statistiques à destination de la communauté.\nLa restitution se fait sous forme de site web. Le site de l’édition 2024 sera disponible ici"
  }
]